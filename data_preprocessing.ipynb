{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQrg3/Ko+2YNEwIHmhwVhg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tribeop/ML-from-scratch/blob/main/data_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39466b09"
      },
      "source": [
        "## 1. Data Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUK2ovMzmWwX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/Data.csv\")\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Numerical Imputation\n",
        "This step addresses missing values in numerical columns like 'Age' and 'Salary' by applying median imputation. The SimpleImputer is initialized with a median strategy, ensuring that missing values are replaced with the median of their respective columns.\n"
      ],
      "metadata": {
        "id": "5_Z8uTOE9onq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "num_cols = [\"Age\", \"Salary\"]\n",
        "\n",
        "imputer = SimpleImputer(strategy=\"median\")\n",
        "df_num_imputed = df.copy()\n",
        "df_num_imputed[num_cols] = imputer.fit_transform(df[num_cols])\n",
        "\n",
        "print(f\"original_df: {df[num_cols]}\")\n",
        "print(f\"imputed_df: {df_num_imputed[num_cols]}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "IfGXgjqOsO8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63f59edb"
      },
      "source": [
        "## 3. One-Hot Encoding\n",
        "\n",
        "This step converts categorical features, specifically the 'Country' column, into a one-hot encoded format. `OneHotEncoder` creates new binary columns for each unique category, effectively transforming nominal data into a numerical representation suitable for machine learning algorithms. The `sparse_output=False` ensures a dense array output, and `handle_unknown=\"ignore\"` prevents errors during transformation if an unseen category appears in the test set. The original 'Country' column is then dropped."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import pandas as pd\n",
        "\n",
        "ohe = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
        "\n",
        "country_ohe = ohe.fit_transform(df_num_imputed[[\"Country\"]])\n",
        "ohe_cols = ohe.get_feature_names_out([\"Country\"])\n",
        "\n",
        "df_country_ohe = pd.DataFrame(country_ohe, columns=ohe_cols, index=df.index)\n",
        "df_encoded = pd.concat([df_num_imputed, df_country_ohe], axis=1)\n",
        "\n",
        "df_encoded = df_encoded.drop(columns=\"Country\")\n",
        "display(df_encoded)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "oUHiR4eA7nw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8d158b9"
      },
      "source": [
        "## 4. Train-Test Split\n",
        "\n",
        "This crucial step divides the dataset into training and testing subsets. The training set (80%) is used to train the machine learning model, while the testing set (20%) is reserved for evaluating the model's performance on unseen data. This split helps assess the model's generalization capabilities and prevent overfitting. `random_state` ensures reproducibility, and `stratify=y` ensures that the proportion of target classes is similar in both training and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df_encoded.drop(columns=\"Purchased\")\n",
        "y = df_encoded[\"Purchased\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n",
        "\n",
        "print(\"X_train head:\\n\", X_train.head())\n",
        "print(\"X_test head:\\n\", X_test.head())\n",
        "print(\"y_train head:\\n\", y_train.head())\n",
        "print(\"y_test head:\\n\", y_test.head())"
      ],
      "metadata": {
        "id": "olNvl-soEF8u",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cfc52f5"
      },
      "source": [
        "## 5. Numerical Feature Scaling\n",
        "\n",
        "This step scales numerical features using StandardScaler to ensure that all features contribute equally to the model, preventing features with larger values from dominating. The scaler is fitted on the training data and then used to transform both training and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "num_cols = [\"Age\", \"Salary\"]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train[num_cols])\n",
        "\n",
        "X_train_scaled = X_train.copy()\n",
        "X_test_scaled = X_test.copy()\n",
        "\n",
        "X_train_scaled[num_cols] = scaler.transform(X_train[num_cols])\n",
        "X_test_scaled[num_cols] = scaler.transform(X_test[num_cols])\n",
        "\n",
        "print(\"Scaled X_train numerical features:\\n\", X_train_scaled[num_cols])\n",
        "print(\"Scaled X_test numerical features:\\n\", X_test_scaled[num_cols])"
      ],
      "metadata": {
        "id": "QrD_srGwt3RH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ff8543b"
      },
      "source": [
        "## Full Implementation - Linear Approach\n",
        "\n",
        "This cell illustrates a manual, step-by-step data preprocessing pipeline, explicitly showing each stage of imputation, one-hot encoding, and scaling in a sequential manner."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "df = pd.read_csv(\"/content/Data.csv\")\n",
        "X = df.drop(columns=\"Purchased\")\n",
        "y = df[\"Purchased\"]\n",
        "\n",
        "num_cols = [\"Age\", \"Salary\"]\n",
        "cat_cols = [\"Country\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "num_imp = SimpleImputer(strategy=\"median\")\n",
        "X_train_num = pd.DataFrame(num_imp.fit_transform(X_train[num_cols]),\n",
        "                           columns=num_cols, index=X_train.index)\n",
        "X_test_num  = pd.DataFrame(num_imp.transform(X_test[num_cols]),\n",
        "                           columns=num_cols, index=X_test.index)\n",
        "\n",
        "ohe = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
        "ohe_train = ohe.fit_transform(X_train[cat_cols])\n",
        "ohe_test  = ohe.transform(X_test[cat_cols])\n",
        "ohe_cols = ohe.get_feature_names_out(cat_cols)\n",
        "\n",
        "X_train_cat = pd.DataFrame(ohe_train, columns=ohe_cols, index=X_train.index)\n",
        "X_test_cat  = pd.DataFrame(ohe_test,  columns=ohe_cols, index=X_test.index)\n",
        "\n",
        "X_train_prep = pd.concat([X_train_num, X_train_cat], axis=1)\n",
        "X_test_prep  = pd.concat([X_test_num,  X_test_cat],  axis=1)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train_prep[num_cols])\n",
        "\n",
        "X_train_prep[num_cols] = scaler.transform(X_train_prep[num_cols])\n",
        "X_test_prep[num_cols]  = scaler.transform(X_test_prep[num_cols])\n",
        "\n",
        "print(\"Train prepared shape:\", X_train_prep.shape)\n",
        "display(X_train_prep.head())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "oqFh3t3hvVfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cef35d06"
      },
      "source": [
        "## Full Implementation - Pipeline Approach\n",
        "\n",
        "This cell demonstrates a streamlined and robust data preprocessing pipeline using Scikit-learn's `Pipeline` and `ColumnTransformer` for efficient handling of numerical and categorical features."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "df = pd.read_csv(\"/content/Data.csv\")\n",
        "\n",
        "y = df[\"Purchased\"].map({\"No\": 0, \"Yes\": 1})\n",
        "X = df.drop(columns=\"Purchased\")\n",
        "\n",
        "num_cols = [\"Age\", \"Salary\"]\n",
        "cat_cols = [\"Country\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "num_pipe = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "cat_pipe = Pipeline([\n",
        "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False, drop=\"first\"))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"num\", num_pipe, num_cols),\n",
        "    (\"cat\", cat_pipe, cat_cols)\n",
        "])\n",
        "\n",
        "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
        "X_test_preprocessed = preprocessor.transform(X_test)\n",
        "\n",
        "feature_names = preprocessor.get_feature_names_out()\n",
        "\n",
        "X_train_df = pd.DataFrame(\n",
        "    X_train_preprocessed,\n",
        "    columns=feature_names,\n",
        "    index=X_train.index\n",
        ")\n",
        "\n",
        "X_test_df = pd.DataFrame(\n",
        "    X_test_preprocessed,\n",
        "    columns=feature_names,\n",
        "    index=X_test.index\n",
        ")\n",
        "\n",
        "print(\"Preprocessed X_train DataFrame head:\")\n",
        "display(X_train_df.head())\n",
        "\n",
        "print(\"\\nPreprocessed X_test DataFrame head:\")\n",
        "display(X_test_df.head())"
      ],
      "metadata": {
        "id": "cWAHvli73Tk1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}